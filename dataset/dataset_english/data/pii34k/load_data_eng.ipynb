{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"ai4privacy/pii-masking-43k\", split='train', streaming=True)\n",
    "dataset_list = list(dataset.take(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Template': 'In our video conference, discuss the role of evidence in the arbitration process involving [FULLNAME_1] and [FULLNAME_2].',\n",
       " 'Filled Template': 'In our video conference, discuss the role of evidence in the arbitration process involving Dr. Marvin Rolfson and Julius Daugherty.',\n",
       " 'Tokenised Filled Template': \"['in', 'our', 'video', 'conference', ',', 'discuss', 'the', 'role', 'of', 'evidence', 'in', 'the', 'arbitration', 'process', 'involving', 'dr', '.', 'marvin', 'rolf', '##son', 'and', 'julius', 'da', '##ugh', '##erty', '.']\",\n",
       " 'Tokens': \"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FULLNAME', 'I-FULLNAME', 'I-FULLNAME', 'I-FULLNAME', 'I-FULLNAME', 'O', 'B-FULLNAME', 'I-FULLNAME', 'I-FULLNAME', 'I-FULLNAME', 'O']\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = load_dataset(\"ai4privacy/pii-masking-43k\", split='train')\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin()\n",
    "label_names = dataset.features['ner_tags'].feature.names\n",
    "def create_spacy_doc(example):\n",
    "    words = example['tokens']\n",
    "    labels = example['ner_tags']\n",
    "    doc = nlp.make_doc(\" \".join(words))\n",
    "    ents = []\n",
    "    for start, label in enumerate(labels):\n",
    "        if label != 0:  \n",
    "            end = start + 1\n",
    "            ent = doc.char_span(doc[start].idx, doc[end - 1].idx + len(doc[end - 1]), label=label_names[label])\n",
    "            if ent:\n",
    "                ents.append(ent)\n",
    "    doc.ents = ents\n",
    "    return doc\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    doc = create_spacy_doc(example)\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"train.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 8248.58it/s]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def create_spacy_docs(dataset_list):\n",
    "    docs = []\n",
    "    for record in tqdm(dataset_list):\n",
    "        text = record['Filled Template']\n",
    "        tokens = record['Tokens']\n",
    "        labels = record['Tokenised Filled Template'].split()\n",
    "        \n",
    "        entities = []\n",
    "        start = 0\n",
    "        for token, label in zip(tokens, labels):\n",
    "            start = text.find(token, start)\n",
    "            end = start + len(token)\n",
    "            if label != 'O':  \n",
    "                entities.append((start, end, label))\n",
    "            start = end\n",
    "        \n",
    "        doc = nlp.make_doc(text)\n",
    "        spans = [doc.char_span(start, end, label=label) for start, end, label in entities]\n",
    "        filtered_spans = filter_spans([span for span in spans if span is not None])\n",
    "        doc.ents = filtered_spans\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "docs = create_spacy_docs(dataset_list)\n",
    "doc_bin = DocBin(docs=docs)\n",
    "doc_bin.to_disk(\"./train.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = create_spacy_docs(dataset['train'])\n",
    "dev_docs = create_spacy_docs(dataset['validation'])\n",
    "\n",
    "# Save to disk\n",
    "DocBin(docs=train_docs).to_disk(\"./train.spacy\")\n",
    "DocBin(docs=dev_docs).to_disk(\"./dev.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy --output ./output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Template  \\\n",
      "0  In our video conference, discuss the role of e...   \n",
      "1  Could you draft a letter for [NAME_1] to send ...   \n",
      "2  Discuss the options for [FULLNAME_1] who wants...   \n",
      "3  13. Write a press release announcing [FULLNAME...   \n",
      "4  9. Develop an inventory management plan for [F...   \n",
      "\n",
      "                                     Filled Template  \\\n",
      "0  In our video conference, discuss the role of e...   \n",
      "1  Could you draft a letter for Dietrich, Schulis...   \n",
      "2  Discuss the options for Jeffery Pfeffer who wa...   \n",
      "3  13. Write a press release announcing Gayle Wat...   \n",
      "4  9. Develop an inventory management plan for Ev...   \n",
      "\n",
      "                           Tokenised Filled Template  \\\n",
      "0  ['in', 'our', 'video', 'conference', ',', 'dis...   \n",
      "1  ['could', 'you', 'draft', 'a', 'letter', 'for'...   \n",
      "2  ['discuss', 'the', 'options', 'for', 'jeff', '...   \n",
      "3  ['13', '.', 'write', 'a', 'press', 'release', ...   \n",
      "4  ['9', '.', 'develop', 'an', 'inventory', 'mana...   \n",
      "\n",
      "                                              Tokens  \n",
      "0  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
      "1  ['O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'I-NA...  \n",
      "2  ['O', 'O', 'O', 'O', 'B-FULLNAME', 'I-FULLNAME...  \n",
      "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FULLNAM...  \n",
      "4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FU...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('PII43k.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42760/42760 [00:10<00:00, 4110.76it/s]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# Initialize a blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create a DocBin object\n",
    "doc_bin = DocBin()\n",
    "\n",
    "# Iterate over the dataframe\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text = row['Filled Template']\n",
    "    tokens = ast.literal_eval(row['Tokenised Filled Template'])\n",
    "    labels = ast.literal_eval(row['Tokens'])\n",
    "    \n",
    "    # Create a Doc object\n",
    "    doc = nlp.make_doc(text)\n",
    "    \n",
    "    ents = []\n",
    "    current_position = 0\n",
    "    for token, label in zip(tokens, labels):\n",
    "        token_start = text.find(token, current_position)\n",
    "        token_end = token_start + len(token)\n",
    "        current_position = token_end\n",
    "        if label != 'O':  # 'O' means no entity\n",
    "            span = doc.char_span(token_start, token_end, label=label)\n",
    "            if span is not None:\n",
    "                ents.append(span)\n",
    "    \n",
    "    # Filter out overlapping spans\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    \n",
    "    # Set entities for the doc\n",
    "    doc.ents = filtered_ents\n",
    "    \n",
    "    # Add the doc to the DocBin\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "# Save the DocBin to a file\n",
    "doc_bin.to_disk(\"train.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paths]\n",
      "train = null\n",
      "dev = null\n",
      "vectors = null\n",
      "init_tok2vec = null\n",
      "\n",
      "[system]\n",
      "gpu_allocator = null\n",
      "seed = 0\n",
      "\n",
      "[nlp]\n",
      "lang = \"en\"\n",
      "pipeline = [\"tok2vec\",\"ner\"]\n",
      "batch_size = 1000\n",
      "disabled = []\n",
      "before_creation = null\n",
      "after_creation = null\n",
      "after_pipeline_creation = null\n",
      "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\n",
      "vectors = {\"@vectors\":\"spacy.Vectors.v1\"}\n",
      "\n",
      "[components]\n",
      "\n",
      "[components.ner]\n",
      "factory = \"ner\"\n",
      "incorrect_spans_key = null\n",
      "moves = null\n",
      "scorer = {\"@scorers\":\"spacy.ner_scorer.v1\"}\n",
      "update_with_oracle_cut_size = 100\n",
      "\n",
      "[components.ner.model]\n",
      "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
      "state_type = \"ner\"\n",
      "extra_state_tokens = false\n",
      "hidden_width = 64\n",
      "maxout_pieces = 2\n",
      "use_upper = true\n",
      "nO = null\n",
      "\n",
      "[components.ner.model.tok2vec]\n",
      "@architectures = \"spacy.Tok2VecListener.v1\"\n",
      "width = ${components.tok2vec.model.encode.width}\n",
      "upstream = \"*\"\n",
      "\n",
      "[components.tok2vec]\n",
      "factory = \"tok2vec\"\n",
      "\n",
      "[components.tok2vec.model]\n",
      "@architectures = \"spacy.Tok2Vec.v2\"\n",
      "\n",
      "[components.tok2vec.model.embed]\n",
      "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
      "width = ${components.tok2vec.model.encode.width}\n",
      "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
      "rows = [5000,1000,2500,2500]\n",
      "include_static_vectors = false\n",
      "\n",
      "[components.tok2vec.model.encode]\n",
      "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
      "width = 96\n",
      "depth = 4\n",
      "window_size = 1\n",
      "maxout_pieces = 3\n",
      "\n",
      "[corpora]\n",
      "\n",
      "[corpora.dev]\n",
      "@readers = \"spacy.Corpus.v1\"\n",
      "path = ${paths.dev}\n",
      "max_length = 0\n",
      "gold_preproc = false\n",
      "limit = 0\n",
      "augmenter = null\n",
      "\n",
      "[corpora.train]\n",
      "@readers = \"spacy.Corpus.v1\"\n",
      "path = ${paths.train}\n",
      "max_length = 0\n",
      "gold_preproc = false\n",
      "limit = 0\n",
      "augmenter = null\n",
      "\n",
      "[training]\n",
      "dev_corpus = \"corpora.dev\"\n",
      "train_corpus = \"corpora.train\"\n",
      "seed = ${system.seed}\n",
      "gpu_allocator = ${system.gpu_allocator}\n",
      "dropout = 0.1\n",
      "accumulate_gradient = 1\n",
      "patience = 1600\n",
      "max_epochs = 0\n",
      "max_steps = 20000\n",
      "eval_frequency = 200\n",
      "frozen_components = []\n",
      "annotating_components = []\n",
      "before_to_disk = null\n",
      "before_update = null\n",
      "\n",
      "[training.batcher]\n",
      "@batchers = \"spacy.batch_by_words.v1\"\n",
      "discard_oversize = false\n",
      "tolerance = 0.2\n",
      "get_length = null\n",
      "\n",
      "[training.batcher.size]\n",
      "@schedules = \"compounding.v1\"\n",
      "start = 100\n",
      "stop = 1000\n",
      "compound = 1.001\n",
      "t = 0.0\n",
      "\n",
      "[training.logger]\n",
      "@loggers = \"spacy.ConsoleLogger.v1\"\n",
      "progress_bar = false\n",
      "\n",
      "[training.optimizer]\n",
      "@optimizers = \"Adam.v1\"\n",
      "beta1 = 0.9\n",
      "beta2 = 0.999\n",
      "L2_is_weight_decay = true\n",
      "L2 = 0.01\n",
      "grad_clip = 1.0\n",
      "use_averages = false\n",
      "eps = 0.00000001\n",
      "learn_rate = 0.001\n",
      "\n",
      "[training.score_weights]\n",
      "ents_f = 1.0\n",
      "ents_p = 0.0\n",
      "ents_r = 0.0\n",
      "ents_per_type = null\n",
      "\n",
      "[pretraining]\n",
      "\n",
      "[initialize]\n",
      "vectors = ${paths.vectors}\n",
      "init_tok2vec = ${paths.init_tok2vec}\n",
      "vocab_data = null\n",
      "lookups = null\n",
      "before_init = null\n",
      "after_init = null\n",
      "\n",
      "[initialize.components]\n",
      "\n",
      "[initialize.tokenizer]\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy init fill-config config.cfg  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "base_config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train base_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy init fill-config base_config.cfg base_config.cfg "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
